{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport os \nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nimport spacy\nimport string\nimport re\nfrom nltk.corpus import sentiwordnet as swn\nimport time\nimport itertools\nimport operator\nfrom gensim import corpora,models\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "DIR_PATH = '../input'\ndata = pd.read_csv('{}/{}'.format(DIR_PATH,'train.csv'))\ntest_data = pd.read_csv('{}/{}'.format(DIR_PATH,'test.csv'))\nsample = data.iloc[:100]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "_cell_guid": "d0293878-6a1f-4c7f-8113-1b6ee1945b9c",
        "_uuid": "d97e5873a9d212b4d1471f99a2b085cec00da021",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "STOPWORDS = stopwords.words('english')\nnlp = spacy.load('en_core_web_sm')\n\ndef tokenize_text(row,col) : \n    ar = nltk.word_tokenize(row[col])\n    return [i.lower() for i in ar]\n\n\ndef remove_stopwords(row,col) : \n    token_set = set(row[col])\n    sw_set = set(STOPWORDS)\n    return list(token_set.difference(sw_set))\n\ndef text_purity(row,col_original,col_new) : \n    original_text = len(row[col_original])\n    new_text = len(row[col_new])\n    return int(new_text)/int(original_text)\n    \ndef stemming(row,col) : \n    stemmer = nltk.stem.WordNetLemmatizer()\n    #stemmer = nltk.stem.snowball.PorterStemmer()\n    return [stemmer.lemmatize(w) for w in row[col]]\n    \ndef stem_word_count(row,col) : \n    return len(row[col].values.tolist())\n\ndef tag_tokens(row,col,tokens=False) : \n    doc = nlp(row[col])\n    return [(t.text,t.tag_) for t in doc]\n\ndef remove_characters_after_tokenization(tokens):\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n    return filtered_tokens\n    \ndef remove_punct(row,col) :\n    return list(remove_characters_after_tokenization(row[col]))\n    \ndef extract_keyword_weights(row,col,NUM_TOPICS) : \n    sents=row[col]\n    print(sents)\n    tokens = list(map(lambda x:nltk.word_tokenize(x),sents))\n    lsi = train_lsi_model_gensim(tokens,total_topics=NUM_TOPICS)\n    parse_weighted_lsi_model(lsi)\n    return parse_weighted_lsi_model\n\ndef feature_matrix_fit(data,vect_type,data_col) : \n    \n    input_data = data[data_col].values\n    if vect_type == 'count' : vectorizer = CountVectorizer(min_df=0.0,max_df=1.0,analyzer='word',ngram_range=(1,1))\n    elif vect_type == 'tfidf' :   vectorizer = TfidfVectorizer(min_df=0.0,max_df=1.0,analyzer='word',ngram_range=(1,1))   \n    features = vectorizer.fit_transform(input_data)\n    return vectorizer,features\n    \ndef feature_engineering(data) : \n    \n    ## input should be a df\n    data['tokens'] = data.apply(tokenize_text,args=('text',),axis=1)\n    data['tokens'] = data.apply(remove_punct,args=('tokens',),axis=1)\n    data['token_string'] = data.apply(lambda x:' '.join(x['tokens']),axis=1)\n    data['no_sw'] = data.apply(remove_stopwords,args=('tokens',),axis=1)\n    data['purity'] = data.apply(text_purity,args=('tokens','no_sw',),axis=1)\n    data['no_sw_string'] = data.apply(lambda x:' '.join(x['no_sw']),axis=1)\n    data['stemmed'] = data.apply(stemming,args=('no_sw',),axis=1)\n    data['wc_stemmed'] = data.apply(lambda x:len(x['stemmed']),axis=1)\n    data['stemmed_string'] = data.apply(lambda x:' '.join(x['stemmed']),axis=1)\n    \n    \n    ## SCALING\n    scaler = MinMaxScaler(feature_range=(0,1))\n    wc_stemmed_feature = scaler.fit_transform(data['wc_stemmed'].reshape(-1,1))\n    data['wc_stemmed_scaled'] = wc_stemmed_feature\n    \n    encoder = LabelEncoder()\n    e=encoder.fit_transform(data['author'].values)\n    data['author_en'] = e              \n    return data\n    \ntrain_data = feature_engineering(data)\n\n## REMOVING OUTLIERS\ntrain_data = train_data[train_data['wc_stemmed']<40]\nvect,train_features = feature_matrix_fit(train_data,vect_type='count',data_col='stemmed_string')\n\n## MANUALLY ADDING FEATURES\n\nprint(train_features.shape)\nprint(train_data['wc_stemmed_scaled'].shape)\n\ntrain_features = np.append(train_features.toarray(),train_data['wc_stemmed_scaled'].reshape(-1,1),axis=1)\ntrain_features = np.append(train_features,train_data['purity'].reshape(-1,1),axis=1)\n\ntrain_labels = train_data['author']\nxtrain,xtest,ytrain,ytest = train_test_split(train_features,train_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e610e9f9-633e-4a7b-818c-76f0d7ab7899",
        "_uuid": "2ebf85f90938c0c0aa28c6c13ceaba26d5bbb7d9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "4e465a57-2e9a-4367-a80a-d84db8e41ae9",
        "_uuid": "8ecc486cc699aa2ebb2ba340049fe14583b870a8",
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def train_lsi_model_gensim(corpus, total_topics=2):\n    dictionary = corpora.Dictionary(corpus)\n    mapped_corpus = [dictionary.doc2bow(text) for text in corpus]\n    tfidf = models.TfidfModel(mapped_corpus)\n    corpus_tfidf = tfidf[mapped_corpus]\n    lsi = models.LsiModel(corpus_tfidf,id2word=dictionary,num_topics=total_topics)\n    return lsi\n\ndef parse_weighted_lsi_model(lsi_model) :\n    TOPIC_ARRAY = []\n    def parse_lsi_eq(eq) : \n        REGEX=r'([\\-0-9\\.]+)\\*\\\"([a-z]+)\\\"'\n        return re.findall(REGEX,eq)      \n    output = lsi_model.print_topics()\n    for t in output : \n        topic_num=t[0]\n        topic_eq=t[1]\n        TOPIC_ARRAY.append(parse_lsi_eq(topic_eq))\n    return TOPIC_ARRAY\n\ndef extract_keyword_weights(row,col,NUM_TOPICS) : \n    sent=row[col]\n    tokens = nltk.word_tokenize(sent)\n    print(tokens)\n    lsi = train_lsi_model_gensim([tokens],total_topics=NUM_TOPICS)\n    return parse_weighted_lsi_model(lsi)\n\n\n#sample_data= data.iloc[2:4]\n#sample_data['keywords'] = sample_data.apply(extract_keyword_weights,args=('token_string',NUM_TOPICS),axis=1)\n#sample_data[['token_string','keywords']].values\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "16d8af66-889b-442e-8629-b247cf154340",
        "_uuid": "ecfdf37171b7710619e3f6468132ef16c1fda175",
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def cosine_distance(u,v) : \n    dist = 1.0 - (np.dot(u,v)/(np.sqrt(sum(np.square(u))) * np.sqrt(sum(np.square(v)))))\n    return dist\n    \ndef test_similarity_scores(train_feature_matrix,test_vector) : \n    y=test_vector\n    return list(map(lambda x:cosine_distance(x,y),train_feature_matrix))\n\ndef label_closeness(N,xtrain,ytrain,test_vector) :\n    ## returns average closness to each class label\n    \n    scores=test_similarity_scores(xtrain[:N],test_vector)\n    df = pd.DataFrame([[sc,aut] for sc,aut in zip(scores,ytrain[:N])],columns=['dist_score','author'])\n    v = df.groupby('author').mean().values\n    return v\n\ndef feature_matrix_fit(data,vect_type,data_col) : \n    \n    input_data = data[data_col].values\n    if vect_type == 'count' : \n        vectorizer = CountVectorizer(min_df=0.0,max_df=1.0,analyzer='word',ngram_range=(1,1))\n    elif vect_type == 'tfidf' : \n        vectorizer = TfidfVectorizer(min_df=0.0,max_df=1.0,analyzer='word',ngram_range=(1,1))\n        \n    features = vectorizer.fit_transform(input_data)\n    return vectorizer,features\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3feedb43-e12b-48b5-a239-cb52fd1c6907",
        "_uuid": "f80f7f15cf7b0df52295eb850f5099cabd29cdcc",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "989c18ed-ac9e-4f31-b201-cceb5047086a",
        "_uuid": "e8793e1ddbb7f5dc8da74235d6e669753b125b6f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c9a3dfca-bb0d-4e6a-a015-4d37e2d1401d",
        "_uuid": "d684755fac4f3df8cca31192664e96268d805b2c"
      },
      "cell_type": "markdown",
      "source": "**COLLOCATIONS FINDING FROM NO_SW TOKENS**"
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "4310dd4e-b652-492e-9b69-0515f7455fcf",
        "_uuid": "868152c7b97797c297346df93073f665bbf51242",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from nltk.collocations import BigramCollocationFinder,TrigramCollocationFinder\nfrom nltk.collocations import BigramAssocMeasures,TrigramAssocMeasures\nimport string\n\n\ntokens=nltk.word_tokenize(' '.join(train_data['no_sw_string'].values))\nfinder = TrigramCollocationFinder.from_words(tokens)\nmeasures = TrigramAssocMeasures()\nfinder.nbest(measures.pmi,10)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2e65fd1b-3f51-4f97-babc-f1a251fa48c4",
        "_uuid": "6bad3ce7388cada2234745e6cc9b81a456e5c636"
      },
      "cell_type": "markdown",
      "source": "**LSI MODEL WITH SVD AND TF-IDF BASED MATRIX**"
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "ee3234c5-4718-49ca-9dda-7fd343af04c9",
        "_uuid": "86870cb8b1dd244b6749cdbd94cb60cc3331caf1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}'):\n    ## sentences is list of sentences\n    SW_LIST=stopwords.words('english')\n    all_chunks = []\n    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n    \n    def key_func(TUPLE) : \n        chunk = TUPLE[2]\n        if chunk!='O': \n            return chunk \n        \n    for sentence in sentences:\n        tagged_sents = nltk.pos_tag_sents([nltk.word_tokenize(sentence)])\n        chunks = [chunker.parse(tagged_sent)for tagged_sent in tagged_sents]\n        wtc_sents = [nltk.chunk.tree2conlltags(chunk) for chunk in chunks]    \n        flattened_chunks = list( itertools.chain.from_iterable(wtc_sent for wtc_sent in wtc_sents) )    \n        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk  in itertools.groupby(flattened_chunks,key_func)]\n        valid_chunks = [' '.join(word.lower()  for word, tag, chunk in wtc_group if word.lower() not in SW_LIST) for status, wtc_group in valid_chunks_tagged if status]\n        all_chunks.append(valid_chunks)\n    \n    return all_chunks\n\ndef get_tfidf_weighted_keyphrases(sentences,grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',top_n=10):\n\n    valid_chunks = get_chunks(sentences, grammar=grammar)\n    dictionary = corpora.Dictionary(valid_chunks)\n   # print('dict : {}'.format(dictionary))\n    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n    #print('corpus : {}'.format(corpus))\n    tfidf = models.TfidfModel(corpus)\n    #print('tfidf : {}'.format(tfidf))\n    corpus_tfidf = tfidf[corpus]\n    #print('corpus_tfidf : {}'.format(corpus_tfidf))\n    weighted_phrases = {dictionary.get(id): round(value,3) for doc in corpus_tfidf for id, value in doc}\n    weighted_phrases = sorted(weighted_phrases.items(),key=operator.itemgetter(1), reverse=True)\n    #print('weighted : {}'.format(weighted_phrases))\n    return weighted_phrases[:top_n]\n\n\ndef train_lsi_model_gensim(corpus, total_topics=2):\n    norm_tokenized_corpus = corpus\n    dictionary = corpora.Dictionary(norm_tokenized_corpus)\n    mapped_corpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\n    tfidf = models.TfidfModel(mapped_corpus)\n    corpus_tfidf = tfidf[mapped_corpus]\n    lsi = models.LsiModel(corpus_tfidf,id2word=dictionary,num_topics=total_topics)\n    return lsi\n\ndef parse_weighted_lsi_model(lsi_model) :\n    TOPIC_ARRAY = []\n    def parse_lsi_eq(eq) : \n        REGEX=r'([\\-0-9\\.]+)\\*\\\"([a-z]+)\\\"'\n        return re.findall(REGEX,eq)      \n    output = lsi.print_topics()\n    for t in output : \n        topic_num=t[0]\n        topic_eq=t[1]\n        TOPIC_ARRAY.append(parse_lsi_eq(topic_eq))\n    return TOPIC_ARRAY",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "f518006f-da7b-4ed1-8964-2b2937b33869",
        "_uuid": "72cbdcca10ca7e1642e14194d864dfd31aa2cdb3",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "toy_text = \"\"\"\nElephants are large mammals of the family Elephantidae\nand the order Proboscidea. Two species are traditionally recognised,\nthe African elephant and the Asian elephant. Elephants are scattered\nthroughout sub-Saharan Africa, South Asia, and Southeast Asia. Male\nAfrican elephants are the largest extant terrestrial animals. All\nelephants have a long trunk used for many purposes,\nparticularly breathing, lifting water and grasping objects. Their\nincisors grow into tusks, which can serve as weapons and as tools\nfor moving objects and digging. Elephants' large ear flaps help\nto control their body temperature. Their pillar-like legs can\ncarry their great weight. African elephants have larger ears\nand concave backs while Asian elephants have smaller ears\nand convex or level backs.\n\"\"\"\nNUM_TOPICS=2\n\ndef extract_keyword_weights(row,col,NUM_TOPICS) : \n    sents=row[col].values\n    tokens = list(map(lambda x:nltk.word_tokenize(x),sents))\n    lsi = train_lsi_model_gensim(tokens,total_topics=NUM_TOPICS)\n    \n    return parse_weighted_lsi_model(lsi)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ce3bf9d9-f62a-4875-92e3-30b9cad65816",
        "_uuid": "66df6ac575f89316cbc14c9ebd07a0c74f04bf25",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "get_tfidf_weighted_keyphrases(train_data['stemmed_string'].values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "3f156785-8c66-4d8c-b2a5-06d3e4304e68",
        "_uuid": "dba9fd67d5837c4bef7bd5b2b8577d3b8bd6314a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def parse_lsi_eq(eq) : \n        REGEX=r'([\\-0-9\\.]+)\\*\\\"([a-z]+)\\\"'\n        return re.findall(REGEX,eq)\nx=parse_lsi_eq(output[0][1])\nx",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "46b3fc5b-a66a-4c52-b0b6-2ff609f9ef01",
        "_uuid": "b203a228bf207727a94661cbfaebef74b9f91842",
        "trusted": false
      },
      "cell_type": "code",
      "source": "ALPHA=np.arange(0,1,0.1)\nALPHA_SCORE=[]\nfor alpha in ALPHA : \n    MNB_model = MultinomialNB(alpha=alpha,fit_prior=False)\n    MNB_model.fit(xtrain,ytrain)\n    score2=MNB_model.score(xtest,ytest)\n    ALPHA_SCORE.append(score2)\n    #print('Multinomial NB = {}, alpha={}'.format(score2,alpha))\n    \nax=plt.plot(ALPHA,ALPHA_SCORE)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "c9a09684-8c44-4c92-867e-859509191517",
        "_uuid": "d480a70d3ee3fde08eae310af29fc3c1fa44e522",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#svm = SGDClassifier(loss='hinge',n_iter=100)\n#svm.fit(xtrain,ytrain)\n#score3=svm.score(xtest,ytest)\n#print('SVM = {}'.format(score3))\nlogr=LogisticRegression(\n        fit_intercept=True,\n        random_state=0,\n        max_iter=300,\n        multi_class='ovr',\n        n_jobs=1\n)\n\nlogr.fit(xtrain,ytrain)\nlogr.score(xtest,ytest)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "300f39cb-3559-4403-a32a-f0cf90dee8ff",
        "_uuid": "86ba363febf33e34340df7d6f25d2b3b77d72e4f",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ax1=train_data[['author','wc_stemmed']].boxplot(by='author',figsize=(15,10))\n\neap = train_data[train_data['author']=='EAP'][['purity','wc_stemmed_scaled']]\nhpl = train_data[train_data['author']=='HPL'][['purity','wc_stemmed_scaled']]\nmws = train_data[train_data['author']=='MWS'][['purity','wc_stemmed_scaled']]\nax2=eap.plot.scatter(x='purity',y='wc_stemmed_scaled',color='Red',figsize=(15,10),label='EAP')\nhpl.plot.scatter(x='purity',y='wc_stemmed_scaled',color='Green',figsize=(15,10),label='HPL',ax=ax2)\nmws.plot.scatter(x='purity',y='wc_stemmed_scaled',color='Blue',figsize=(15,10),label='MWS',ax=ax2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "bbf49e92-6fe1-4bea-bd44-3fea32972774",
        "_uuid": "b0332240af70b34c604bb5a178bd8fd185314ae9",
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "61584085-cfbb-4e71-bd6f-1db8a23b0e4f",
        "_uuid": "0e207dafcdde0f31705ae78455adfa7a049af6b9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "0254e4db-24af-4ac2-b79f-9416dad2d85a",
        "_uuid": "3503be9f0bfeff124bb4417f4f09e715457d193e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "d3fc423e-bc23-49fb-b025-7e6f6835bcbb",
        "_uuid": "30d06f3bb2b92c1fed4a192caff4cee7776d32bc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "3dcc23e7-2ce7-4b50-a98e-6e7adfe3834e",
        "_uuid": "98153bdf92662c4ed85bb1db3cacea268ee9bfa1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}